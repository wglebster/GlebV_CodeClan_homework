---
title: "Model Building homework"
author: "GlebV"
output: html_notebook
---
```{r}
library(tidyverse)
```


### Get and prepare  data

Get and inspect the data.
```{r}
avocado <- read_csv("data/avocado.csv")
glimpse(avocado)
```
```{r}
library(lubridate)
library(janitor)
```

Tidy up data:

I defined date_of_month and month as factors variables, as they might provide some insight. I also converted year as.factor, as year does not represent continuous data. 
```{r}
clean_avocado <- avocado  %>%
  mutate(month = as.factor(month(Date)), 
         day_of_month = as.factor(mday(Date)),
         year = as.factor(year)) %>%
  clean_names() %>%
  select(-c(x1, date))

glimpse(clean_avocado)
```
```{r}
#check for NA values
summary(clean_avocado)
```

Check for aliased variables in the dataset:

```{r}
alias(lm(average_price ~ . , data = clean_avocado))
```
It looks like there arent any variables that are aliased, according to the code. 

However it is possible to work out the number of small/large/x_large bags based on total_bags. 

#### test_1_model

* As step one I will check model statisticks with all the features included in it.
```{r}
test_1_model <- lm(average_price ~ . , data = clean_avocado)
summary(test_1_model)
```

***all variables***
```{r}
library(broom)
```

```{r}
0.239/mean(clean_avocado$average_price)*100
glance(test_1_model)
```
I think that an error that is 17% of the mean avocado price is quite high, I would also prefer ARS higher than 0.6478 

But lets check the diagnostics: 
```{r}
par(mfrow = c(2,2))
plot(test_1_model)
```
Data seems quite well ditributed. There are some outliers, but they don't seem to influence the data to dramatically. 

I will be using following dataframe to compare all Persimonious diagnostic values to help me choose the most suitable explanatory model. 
```{r}
#I can have an all model comparisons in a single tibble. 
#I can use add_row to add data.
model_compare <- tibble(model = "test_1_model",
                        Adj_R_sq = round(digits = 4,summary(test_1_model)$adj.r.squared),
                        AIC = round(digits = 4, AIC(test_1_model)),
                        BIC = round(digits = 4, BIC(test_1_model))
                        )
model_compare
```

***I will limit my model to 4 features***

To try and keep it simple, I will limit my model to max 4 features. I will check if including interactions helps to improve the model performance. 

I found that it may be helpful to compare T-values in the overfitted model to get a general feel of which features might be helpful for manual model building.

#### model_1

```{r}
model_1 <- lm(average_price ~ type + 
                year + month + day_of_month + region, data = clean_avocado)
summary(model_1)
```

Checking parsimonious numbers, model_1 performs worse than overfitted test_1_model. 

```{r}
model_compare <- model_compare %>% 
  add_row(model = "model_1", 
          Adj_R_sq = round(digits = 4, summary(model_1)$adj.r.squared),
          AIC = round(digits = 4, AIC(model_1)),
          BIC = round(digits = 4, BIC(model_1))
          )
model_compare
```

Check model_1 diagnostic chart

```{r}
par(mfrow = c(2,2))
plot(model_1)
```

The data is spread quite well, but I am concerned about Residuals vs Leverage - too much data on the extreme right.

#### model_2

model_2 has interactions year:month:day_of_month.

```{r}
model_2 <- lm(average_price ~ type + 
                year:month:day_of_month + region, data = clean_avocado)
```
```{r}
model_compare <- model_compare %>% 
  add_row(model = "model_2", 
          Adj_R_sq = round(digits = 4, summary(model_2)$adj.r.squared),
          AIC = round(digits = 4, AIC(model_2)),
          BIC = round(digits = 4, BIC(model_2))
          )
model_compare
```
```{r}
par(mfrow = c(2,2))
plot(model_2)
```
This model has better parsimonious values than is overfitted due to included interactions. 

If I am including year:month:day_of_month interaction, am I better off by perorming time series analysis? I feel like I am overfitting by including day_of_month interaction, as this data is at the most granular level and there is possibility that it doesn't always get recorded.

#### model_3

```{r}
model_3 <- lm(average_price ~ type + 
                year:month + region, data = clean_avocado)
summary(model_3)
```

```{r}
model_compare <- model_compare %>% 
  add_row(model = "model_3", 
          Adj_R_sq = round(digits = 4, summary(model_3)$adj.r.squared),
          AIC = round(digits = 4, AIC(model_3)),
          BIC = round(digits = 4, BIC(model_3))
          )
model_compare
```
```{r}
par(mfrow = c(2,2))
plot(model_3)
```
So far model_3 seems to be the most suitable and year:month are more likely to be recorded than year:month:day_of_month. Also year:month is less granular, therefore I don't feel guilty for potentially overfitting the data. 


#### model_4

```{r}
model_4 <- lm(average_price ~ type:region +  
              year:month, data = clean_avocado)
summary(model_4)
```

```{r}
model_compare <- model_compare %>% 
  add_row(model = "model_4", 
          Adj_R_sq = round(digits = 4, summary(model_4)$adj.r.squared),
          AIC = round(digits = 4, AIC(model_4)),
          BIC = round(digits = 4, BIC(model_4))
          )
model_compare
```

model_4 is the strongest so far, unless I am going overboard :) I wish I took note of R-Sq values in addition to Adj_R_sq.  

```{r}
par(mfrow = c(2,2))
plot(model_4)
```
Norm Q-Q distribution is not great is any of the 6 models, residuals vs leverage are heavy to the right. 

What concerns me the most is that adding interactions, causes formation of data clusters on the extreme right in Residuals vs Fitted and Scale-Location.

#### model_5

```{r}
model_5 <- lm(average_price ~ type:region +  
              month, data = clean_avocado)
summary(model_5)
```
```{r}
model_compare <- model_compare %>% 
  add_row(model = "model_5", 
          Adj_R_sq = round(digits = 4, summary(model_5)$adj.r.squared),
          AIC = round(digits = 4, AIC(model_5)),
          BIC = round(digits = 4, BIC(model_5))
          )
model_compare
```
Removing annual variation from the model changes its parsimonious diagnostic values.

```{r}
par(mfrow = c(2,2))
plot(model_5)
```
However data distribution seems to be the best of all models, except for Residuals vs Leverage.

#### model_6

```{r}
model_6 <- lm(average_price ~ type +  
              year:month, data = clean_avocado)
summary(model_6)
```
```{r}
model_compare <- model_compare %>% 
  add_row(model = "model_6", 
          Adj_R_sq = round(digits = 4, summary(model_6)$adj.r.squared),
          AIC = round(digits = 4, AIC(model_6)),
          BIC = round(digits = 4, BIC(model_6))
          )
model_compare
```
Excluding Region reduces model accuracy, therefore I will not pursue it.

### Further fine tuning and diagnostics. 

I feel that model_4 & model_5 are good contenders to undergo further diagnostics. 

model_4 has very good parsimonious values, but model_5 has the best distribution of data. 
#### Train & Test

create training and testing sets

*model_4
```{r}
n_data <- nrow(clean_avocado) #18249 rows
test_index <- sample(1:n_data, size = n_data * 0.2)

train_set <- slice(clean_avocado, -test_index)
test_set <- slice(clean_avocado, test_index)
```

```{r}
model_4_train <- lm(average_price ~ type:region +  
                    year:month , data = train_set)
model_4_predict <- predict(model_4_train, newdata = test_set)

model_5_train <- lm(average_price ~ type:region +  
                    month, data = train_set)
model_5_predict <- predict(model_5_train, newdata = test_set)
```

model_4 & model_5 train_vs_predict performance.
```{r}
median(abs(model_4_predict - test_set$average_price))
median(abs(model_5_predict - test_set$average_price))
```
Prediction error in model_4 is lower than in model_5. 

In model_4 some intercations cannot be calculated and I am getting NAs. 

This is why I am getting a warning: "prediction from a rank-deficient fit may be misleading". 

Also p-value is over 5% for some type:region interactions, I wonder if transforming data will help overcome this issue?

I will first attemt K-fold validation: 
```{r}
library(caret)
```

```{r}
cv_model_4 <- trainControl(method = "cv",
                            number = 10,
                            savePredictions = TRUE)
model_4_fold <- train(average_price ~ type:region +  
                    year:month, data = clean_avocado,
                    trControl = cv_model_4, method = "lm")
###########
cv_model_5 <- trainControl(method = "cv",
                           number = 10,
                           savePredictions = TRUE)
model_5_fold <- train(average_price ~ type:region +  
              month, data = clean_avocado, trControl = cv_model_5, method = "lm")

```
```{r}
mean(model_4_fold$resample$RMSE)
mean(model_4_fold$resample$Rsquared)
#####
mean(model_5_fold$resample$RMSE)
mean(model_5_fold$resample$Rsquared)
```
***Same as in parsimonious tests model_4 performs better than model_5***

I found it useful to run train&test diagnostic and/or K-fold test, because both of them pointed out that my models may be misleading due to model's inability to calculculate effect of some interactions. 


##Predictive model.

Just going to try using "gmulti" to see again how it works.
```{r}
library(glmulti)
```

```{r}
future_avocado_model <- glmulti(average_price ~ 
                                total_volume + x4046 + 
                                x4225 + x4770 + total_bags + 
                                small_bags + large_bags + 
                                x_large_bags + type + 
                                year + region + month + 
                                day_of_month, data = clean_avocado, 
                                method = "g", level = 1, plotty = FALSE)
```

```{r}
summary(future_avocado_model)
summary(future_avocado_model)$bestmodel

```
```{r}
# relative importance of features
plot(future_avocado_model, type = "s") 
#models below the red line are worth considering, however the algorithm is likely to have chosen the best suitable model
plot(future_avocado_model, type = "p") 
plot(future_avocado_model, type = "w") 
```
Display the list of all models in ranked order from best to worst.
```{r}
weightable(future_avocado_model)
```

So the best model suggested by GLMulti is: average_price ~ year + month + day_of_month + x4225 + x4770 + large_bags + x_large_bags + type + region.

I will now test it: 

```{r}
glmulti_avo_model_1 <- lm(average_price ~ year + month + 
                          day_of_month + x4225 + x4770 + 
                          large_bags + x_large_bags + type + 
                          region, 
                        data = clean_avocado)
summary(glmulti_avo_model_1)
```

glmulti_avo_model_1 (suggested by glmulti). It has R-sq of 0.6498 and adj-R-sq of 0.6478, which suggests to me that it may be overfitted. RSE is 0.239.

```{r}
model_compare <- model_compare %>% 
  add_row(model = "glmulti_avo_model_1", 
          Adj_R_sq = round(digits = 4, summary(glmulti_avo_model)$adj.r.squared),
          AIC = round(digits = 4, AIC(glmulti_avo_model)),
          BIC = round(digits = 4, BIC(glmulti_avo_model))
          )
model_compare

```
```{r}
par(mfrow = c(2,2))
plot(glmulti_avo_model_1)
```
This model also seems to have the best distribution of data. 



# QUIZ

## 1. I want to predict how well 6 year-olds are going to do in their final school exams. Using the following variables am I likely under-fitting, fitting well or over-fitting? Postcode, gender, reading level, score in maths test, date of birth, family income.

If the math test is mandatory for all pupils, this model may be helpful in predicting the final exam results. 

Including date of birth, I believe will tend toward overfitting. We should make sure that the training dataset contains information of 6-year olds only.

## 2. If I have two models, one with an AIC score of 34,902 and the other with an AIC score of 33,559 which model should I use?

Generally speaking ower AIC score is more parsimonious, therefore I'd select the model with score of 34902.

## 3. I have two models, the first with: r-squared: 0.44, adjusted r-squared: 0.43. The second with: r-squared: 0.47, adjusted r-squared: 0.41. Which one should I use?

Generally speaking the higher r-squared/adjusted r-squared values are better. But while r-squared will be increasing with each additional feature, adjusted r-square will converge and start decreasing at some point. The point of convergence indicates that the model is overfitted.

## 4. I have a model with the following errors: RMSE error on test set: 10.3, RMSE error on training data: 10.4. Do you think this model is over-fitting?

Usually it is expected that a model fits to training-set better than to test-set, therefore I expect that the model described is overfitted.

## 5. How does k-fold validation work? 

A dataset is split into K-number of folds. Each fold is used for testing at some point during iterations, the other folds are used to train the model. Iterations continue until each fold has been used as test set. 

## 6. What is a validation set? When do you need one?

A validation set can be used between training and testing. Once a few candidate models have been selected, they could be fine tuned using a validation set, to make sure they do not overfit. The next step is to test the models on a test set to make sure they perform as expected (hoped for). 

## 7. Describe how backwards selection works.

It establishes the most optimal model features by starting from the most overfitted model and working backwards, eliminating features, until the modelbecomes well-fitted.   

## 8. Describe how best subset selection works.

Best subset regression compares all possible models using specified predictors and displays the best fitting models that have x-number of predictors. the output provides a list of models with their summary statistics. 

The alternative is Stepwise Regression, which might be easier to interpret.

## 9. It is estimated on 5% of model projects end up being deployed. What actions can you take to maximise the likelihood of your model being deployed?

Be able to explain what the model does and why it is useful and if it does what it needs to do. Document the model!

Train, Validate, Test. Be able to explain and justify inclusion/exclusion of parameters.  

## 10. What metric could you use to confirm that the recent population is similar to the development population?

mean(pop) - mean(dev_pop)

## 11. How is the Population Stability Index defined? What does this mean in words?

PSI could help in determing if the data has changed significantly and if an existing model is still suitable to use on new data. 

Events such as changes in product characteristics or large socioeconomic events could deem models useless. Model performance is prone to deteriorating over time in general. 

## 12. Above what PSI value might we need to start to consider rebuilding or recalibrating the model. 

If PSI score goes over 0.2 the model needs to be replaced / recalibrated. 

## 13. What are the common errors that can crop up when implementing a model?

https://www.strategywise.com/five-common-errors-to-avoid-when-building-predictive-models/

* Lack of understanding of the business use case. 

* Lack of Data Integrity.

* The model over or under fits. 

* Bias/Variance imbalance.

It is important to build balanced (not too complex and not too simple). Overly complex models will tend to have too much variance in their outputs. Overly simple models are likely to produce biased outputs. 

* Testing and evaluation errors. 

## 14. After performance monitoring, if we find that the discrimination is still satisfactory but the accuracy has deteriorated, what is the recommended action? 

Root cause should be investigated and the model should be recalibrated/rebuilt.

## 15. Why is it important to have a unique model identifier for each model? 

So that models can be identified/distinguished from others. 

## 16. Why is it important to document the modelling rationale and approach?

So that the model can be understood by others, e.g. for audit purposes.
